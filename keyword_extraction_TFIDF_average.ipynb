{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "from tqdm import tqdm \n",
    "from rake_nltk import Rake\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert these dicts to csv file\n",
    "\n",
    "keyword_statistic = {}\n",
    "keyword_statistic[\"keywords\"] = []\n",
    "keyword_statistic[\"review frequency\"] = []\n",
    "keyword_statistic[\"user id\"] = set()\n",
    "\n",
    "user_statistic = {}\n",
    "user_statistic[\"id\"] = []\n",
    "user_statistic[\"keywords\"] = []\n",
    "user_statistic[\"review count\"] = []\n",
    "user_keyword = {}\n",
    "keyword_user = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict_of_reviews_info(city_review_dir):\n",
    "    restaurants_dir_list = os.listdir(city_review_dir)\n",
    "    reviews_info = {}  # dict to store review info by id \n",
    "\n",
    "    for restaurant_dir in tqdm(restaurants_dir_list, total=len(restaurants_dir_list)):\n",
    "        if restaurant_dir.startswith('.'):\n",
    "            continue # macos issue\n",
    "\n",
    "        restaurant_dir = os.path.join(city_review_dir, restaurant_dir)\n",
    "        reviews = json.load(open(restaurant_dir))\n",
    "        reviews_info.update(reviews)\n",
    "\n",
    "    return reviews_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 984/984 [00:08<00:00, 117.62it/s]\n"
     ]
    }
   ],
   "source": [
    "city_review_dir = 'reviews_users/reviews_users/singapore/extracted_reviews'\n",
    "\n",
    "document_frequency = {} # keyword - document frequency\n",
    "review_keywords = {}  # id - dict of (keywords, sentence frequency)\n",
    "\n",
    "# dict with keys are review ids, values are dicts of review info, values' keys are ['date', 'text', 'rating', 'rated', 'photos', 'user_id']\n",
    "reviews_info = create_dict_of_reviews_info(city_review_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'text', 'rating', 'rated', 'photos', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "for id in list(reviews_info.keys()):\n",
    "    print(list(reviews_info[id].keys()))\n",
    "    break\n",
    "\n",
    "def load_stopwords(ifile):\n",
    "    with open(ifile, 'r') as reader:\n",
    "        tmp = [a.strip() for a in reader.readlines()]  #strip to remove space\n",
    "\n",
    "    return set(tmp)\n",
    "\n",
    "def word_only_tokenizer(txt):\n",
    "    tmp = nltk.tokenize.wordpunct_tokenize(txt)\n",
    "    return [a for a in tmp if a.isalpha()]\n",
    "\n",
    "def init_raker_nltk(stop_file='Stopwords.txt', word_tokenizer=word_only_tokenizer):\n",
    "    stopwords = load_stopwords(stop_file)\n",
    "    return Rake(stopwords=stopwords, word_tokenizer=word_tokenizer)\n",
    "\n",
    "def ext_keywords_rake_nltk(text, raker_nltk): # return a list of tuples\n",
    "    raker_nltk.extract_keywords_from_text(text)\n",
    "    return raker_nltk.get_ranked_phrases_with_scores()\n",
    "\n",
    "def extract_keywords_from_review(review_info, raker_nltk):\n",
    "    tk_with_scores = ext_keywords_rake_nltk(review_info['text'], raker_nltk) # list of tuples\n",
    "    return tk_with_scores\n",
    "\n",
    "\n",
    "def extract_and_count_keywords_for_city(reviews_info, raker_nltk): # -> review_keyword, document_frequency\n",
    "    for review_id in list(reviews_info.keys()):\n",
    "        tk_with_score = extract_keywords_from_review(reviews_info[review_id], raker_nltk)\n",
    "        review_keywords[review_id] = {}\n",
    "\n",
    "        for score, kw in tk_with_score:\n",
    "            if kw not in review_keywords[review_id]:\n",
    "                document_frequency[kw] = document_frequency.get(kw, 0) + 1\n",
    "                \n",
    "            review_keywords[review_id][kw] = review_keywords[review_id].get(kw, 0) + 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raker_nltk = init_raker_nltk(stop_file='Stopwords.txt', word_tokenizer=word_only_tokenizer)\n",
    "\n",
    "extract_and_count_keywords_for_city(reviews_info, raker_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_keywords = set()\n",
    "\n",
    "TFIDF_Average = {} # keyword : (sum of TFIDF / document frequency)\n",
    "\n",
    "for id in list(review_keywords.keys()):\n",
    "    if len(review_keywords[id]) == 0:\n",
    "        continue\n",
    "\n",
    "    for kw, fre in review_keywords[id].items():\n",
    "        if document_frequency[kw] < 4:\n",
    "            continue\n",
    "        TF = fre / len(review_keywords[id])\n",
    "        IDF = math.log(len(review_keywords)/ document_frequency[kw], 2)\n",
    "        TFIDF_Average[kw] = TFIDF_Average.get(kw, 0) + TF*IDF/document_frequency[kw]\n",
    "\n",
    "\n",
    "\n",
    "sorted_keywords = sorted(TFIDF_Average.items(), key=lambda x:x[1]) \n",
    "\n",
    "# remove keyword with low tf-idf value, add to set_of_keyword\n",
    "for pair in sorted_keywords[::-1]:\n",
    "    if pair[1] < 0.08:\n",
    "        break\n",
    "    set_of_keywords.add(pair[0])\n",
    "\n",
    "# print(set_of_keywords)\n",
    "\n",
    "keyword_statistic[\"keywords\"] = list(set_of_keywords)\n",
    "for kw in keyword_statistic[\"keywords\"]:\n",
    "    keyword_statistic[\"review frequency\"].append(document_frequency[kw])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(user_statistic[\"keywords\"]))\n",
    "def user_keyword_review_count(reviews_info):\n",
    "    user_keyword_count = {}\n",
    "    user_review_count = {}\n",
    "    for id in list(reviews_info.keys()):\n",
    "        user_review_count[reviews_info[id]['user_id']] = user_review_count.get(reviews_info[id]['user_id'], 0) + 1\n",
    "        for kw in review_keywords[id]:\n",
    "            if kw in set_of_keywords: # only consider keyword in set_of_keyword\n",
    "                \n",
    "                if kw not in keyword_user:\n",
    "                    keyword_user[kw] = set()\n",
    "                keyword_user[kw].add(reviews_info[id]['user_id'])\n",
    "\n",
    "                if reviews_info[id]['user_id'] not in user_keyword:\n",
    "                    user_keyword[reviews_info[id]['user_id']] = set()\n",
    "                user_keyword[reviews_info[id]['user_id']].add(kw)\n",
    "                \n",
    "                user_keyword_count[reviews_info[id]['user_id']] = user_keyword_count.get(reviews_info[id]['user_id'], 0) + 1\n",
    "\n",
    "    return (user_keyword_count, user_review_count)\n",
    "\n",
    "(user_keyword_count, user_review_count) = user_keyword_review_count(reviews_info)\n",
    "keyword_statistic[\"user id\"] = list(keyword_statistic[\"user id\"])\n",
    "\n",
    "for kw in keyword_statistic['keywords']:\n",
    "    keyword_statistic[\"user id\"].append(list(keyword_user[kw]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = \"singapore\"\n",
    "dt = json.load(open('users_split_1.json'))\n",
    "\n",
    "# statisticize for user in user_test_split\n",
    "\n",
    "for user_id in dt[city]['test']:\n",
    "    user_statistic[\"id\"].append(user_id)\n",
    "\n",
    "for user_id in dt[city]['train']:\n",
    "    user_statistic[\"id\"].append(user_id)\n",
    "\n",
    "for user_id in dt[city]['val']:\n",
    "    user_statistic[\"id\"].append(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2171\n",
      "2171\n"
     ]
    }
   ],
   "source": [
    "for id in user_statistic[\"id\"]: \n",
    "    user_statistic[\"review count\"].append(user_review_count[id])\n",
    "    if id not in user_keyword:\n",
    "        user_statistic[\"keywords\"].append([])\n",
    "    else:\n",
    "        user_statistic[\"keywords\"].append(list(user_keyword[id]))\n",
    "    # print(id)\n",
    "\n",
    "\n",
    "print(len(user_statistic[\"id\"]))\n",
    "print(len(user_statistic[\"keywords\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'keywords', 'review count'])\n"
     ]
    }
   ],
   "source": [
    "print(user_statistic.keys())\n",
    "df1 = pd.DataFrame(user_statistic)\n",
    "df2 = pd.DataFrame(keyword_statistic)\n",
    "\n",
    "df1.to_csv(r'data_csv/user_statistic.csv', header=True)\n",
    "df2.to_csv(r'data_csv/keyword_statistic.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c46ac5aaab4d3c9f2f9be91c3b9b95f2b8ede1f4aae554ca1e34eefcba1468b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
